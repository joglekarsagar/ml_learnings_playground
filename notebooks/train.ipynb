{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d30412-a7a3-4a00-9232-e7a8ad6eb750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a428ec1c-a0b1-4c27-b8b3-6e3788b9aae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "with open(\"output_data_50.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0832fffc-9870-4a11-a760-38607b199d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paulina:true, too true, my lord:if, one by one_eno yb eno ,fi:drol ym ,eurt oot ,eurt:aniluap^\\n',\n",
       " 'r he make his wayunder the colour of his usual ga_ag lausu sih fo ruoloc eht rednuyaw sih ekam eh r^\\n',\n",
       " 'sages, and in the end,having my freedom, boast of_fo tsaob ,modeerf ym gnivah,dne eht ni dna ,segas^\\n',\n",
       " 'ng henry virichard:now, clifford, i have single_elgnis evah i ,droffilc ,won:drahciriv yrneh gn^\\n',\n",
       " \"-damn him. be she honour-flaw'd,i have three daug_guad eerht evah i,d'walf-ruonoh ehs eb .mih nmad-^\\n\",\n",
       " \" clifford; and that's richard duke of york.king _ gnik.kroy fo ekud drahcir s'taht dna ;droffilc ^\\n\",\n",
       " 'and with your queen. i am his cupbearer:if from _ morf fi:reraebpuc sih ma i .neeuq ruoy htiw dna^\\n',\n",
       " 'le:northumberland comes back from bolingbroke.k_k.ekorbgnilob morf kcab semoc dnalrebmuhtron:el^\\n',\n",
       " ' so unlike yourself?petruchio:tedious it were t_t erew ti suoidet:oihcurtep?flesruoy ekilnu os ^\\n',\n",
       " 'y, proud queen, to make thee blush.to tell thee w_w eeht llet ot.hsulb eeht ekam ot ,neeuq duorp ,y^\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c5621a-3f0b-43bb-98c3-cdccb8ab6431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "with open(\"output_data_50.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "text = \"\".join(sentences[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181c0558-ce2a-4490-a96a-d66e809a3698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device_type)\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcbbb37-4905-4d02-9122-81a4e36777d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_vocab = sorted(list(set(\"\".join(sentences))))\n",
    "vocab_size = len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ca940a-532c-4235-82ea-13178429b0e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 23, 23, 1, 34, 22, 19, 32, 19, 13]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(total_vocab) }\n",
    "itos = { i:ch for i,ch in enumerate(total_vocab) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there^\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff227190-e4e0-46ff-9ee7-0bd75d019234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 20470.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([30, 15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1,\n",
      "        34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1,\n",
      "        29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39, 16,  1,\n",
      "        19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19,\n",
      "        35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26,\n",
      "        35, 15, 30, 13,  0]), tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28, 18, 19, 32,  1, 34, 22, 19,  1, 17, 29, 26, 29, 35, 32,  1, 29, 20,\n",
      "         1, 22, 23, 33,  1, 35, 33, 35, 15, 26,  1, 21, 15, 14, 15, 21,  1, 26,\n",
      "        15, 35, 33, 35,  1, 33, 23, 22,  1, 20, 29,  1, 32, 35, 29, 26, 29, 17,\n",
      "         1, 19, 22, 34,  1, 32, 19, 18, 28, 35, 39, 15, 37,  1, 33, 23, 22,  1,\n",
      "        19, 25, 15, 27,  1, 19, 22,  1, 32, 13,  0]), tensor([33, 15, 21, 19, 33,  6,  1, 15, 28, 18,  1, 23, 28,  1, 34, 22, 19,  1,\n",
      "        19, 28, 18,  6, 22, 15, 36, 23, 28, 21,  1, 27, 39,  1, 20, 32, 19, 19,\n",
      "        18, 29, 27,  6,  1, 16, 29, 15, 33, 34,  1, 29, 20, 14, 20, 29,  1, 34,\n",
      "        33, 15, 29, 16,  1,  6, 27, 29, 18, 19, 19, 32, 20,  1, 39, 27,  1, 21,\n",
      "        28, 23, 36, 15, 22,  6, 18, 28, 19,  1, 19, 22, 34,  1, 28, 23,  1, 18,\n",
      "        28, 15,  1,  6, 33, 19, 21, 15, 33, 13,  0]), tensor([28, 21,  1, 22, 19, 28, 32, 39,  1, 36, 23, 32, 23, 17, 22, 15, 32, 18,\n",
      "        10, 28, 29, 37,  6,  1, 17, 26, 23, 20, 20, 29, 32, 18,  6,  1, 23,  1,\n",
      "        22, 15, 36, 19,  1, 33, 23, 28, 21, 26, 19, 14, 19, 26, 21, 28, 23, 33,\n",
      "         1, 19, 36, 15, 22,  1, 23,  1,  6, 18, 32, 29, 20, 20, 23, 26, 17,  1,\n",
      "         6, 37, 29, 28, 10, 18, 32, 15, 22, 17, 23, 32, 23, 36,  1, 39, 32, 28,\n",
      "        19, 22,  1, 21, 28, 13,  0]), tensor([ 7, 18, 15, 27, 28,  1, 22, 23, 27,  8,  1, 16, 19,  1, 33, 22, 19,  1,\n",
      "        22, 29, 28, 29, 35, 32,  7, 20, 26, 15, 37,  5, 18,  6, 23,  1, 22, 15,\n",
      "        36, 19,  1, 34, 22, 32, 19, 19,  1, 18, 15, 35, 21, 14, 21, 35, 15, 18,\n",
      "         1, 19, 19, 32, 22, 34,  1, 19, 36, 15, 22,  1, 23,  6, 18,  5, 37, 15,\n",
      "        26, 20,  7, 32, 35, 29, 28, 29, 22,  1, 19, 22, 33,  1, 19, 16,  1,  8,\n",
      "        27, 23, 22,  1, 28, 27, 15, 18,  7, 13,  0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = []\n",
    "for line in tqdm(sentences[:10]): \n",
    "    data.append(torch.tensor(encode(line), dtype=torch.long))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bd9304-4a5a-4877-b404-6997eaf015ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45063e57-50f1-4f2e-9363-d916b2cdb145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28, 18]) tensor([ 1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35, 28,\n",
      "        18, 19])\n",
      "when input is tensor([32]) the target: 1\n",
      "when input is tensor([32,  1]) the target: 22\n",
      "when input is tensor([32,  1, 22]) the target: 19\n",
      "when input is tensor([32,  1, 22, 19]) the target: 1\n",
      "when input is tensor([32,  1, 22, 19,  1]) the target: 27\n",
      "when input is tensor([32,  1, 22, 19,  1, 27]) the target: 15\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15]) the target: 25\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25]) the target: 19\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19]) the target: 1\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1]) the target: 22\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22]) the target: 23\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23]) the target: 33\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33]) the target: 1\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1]) the target: 37\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37]) the target: 15\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15]) the target: 39\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39]) the target: 35\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35]) the target: 28\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28]) the target: 18\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28, 18]) the target: 19\n"
     ]
    }
   ],
   "source": [
    "block_size = 20\n",
    "\n",
    "x = train_data[1][:block_size]\n",
    "y = train_data[1][1:block_size+1]\n",
    "print(x, y)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7f2ab5-eb08-4ddb-b573-a1fff675651f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 7, 4, 0, 0, 5, 4, 0, 0, 9, 8, 7, 2, 0, 0, 2, 6, 1, 3, 7, 9, 3, 4, 4,\n",
       "        8, 8, 5, 9, 5, 0, 7, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a90603d-09ed-43e1-a1c5-566b2f141996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 16 # what is the maximum context length for predictions?\n",
    "\n",
    "# poor man's data loader\n",
    "def get_batch(sample):\n",
    "    data = sample\n",
    "    ix = torch.randint(len(data) - block_size + 1, (batch_size,))\n",
    "    x_temps = [data[i:i+block_size] for i in ix]\n",
    "    x = []\n",
    "    for _x in x_temps:\n",
    "        if len(_x) < block_size:\n",
    "            while len(_x) < block_size:\n",
    "                new_element = torch.tensor(encode(\"^\"))\n",
    "                _x = torch.cat((_x,new_element),0)\n",
    "        x.append(_x)\n",
    "    \n",
    "    y_temps = [data[i+1:i+1+block_size] for i in ix]\n",
    "    y = []\n",
    "    for _y in y_temps:\n",
    "        \n",
    "        if len(_y) < block_size:\n",
    "            while len(_y) < block_size:\n",
    "                new_element = torch.tensor(encode(\"^\"))\n",
    "                _y = torch.cat((_y,new_element),0)\n",
    "        y.append(_y)\n",
    "    \n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device_type, non_blocking=True), y.pin_memory().to(device_type, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device_type), y.to(device_type)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd62b4f-dfd6-451c-a7d2-d3ddeb8b1a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1,\n",
       "        34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1,\n",
       "        29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39, 16,  1,\n",
       "        19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19,\n",
       "        35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26,\n",
       "        35, 15, 30, 13,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87f66f5-fde6-4527-a953-9fffe1973c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28],\n",
       "         [39, 27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35],\n",
       "         [29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26, 35, 15],\n",
       "         [34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32],\n",
       "         [ 1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28, 19,  1, 16, 39],\n",
       "         [28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6],\n",
       "         [23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1],\n",
       "         [23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32],\n",
       "         [ 6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10],\n",
       "         [32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23],\n",
       "         [34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20],\n",
       "         [19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1],\n",
       "         [35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39],\n",
       "         [34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26],\n",
       "         [19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15],\n",
       "         [16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39],\n",
       "         [ 1, 29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1],\n",
       "         [28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6],\n",
       "         [10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6],\n",
       "         [ 1, 39, 27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19],\n",
       "         [ 1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34],\n",
       "         [29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39],\n",
       "         [20,  6,  1, 29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28],\n",
       "         [19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15],\n",
       "         [20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34],\n",
       "         [ 1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29],\n",
       "         [19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29],\n",
       "         [29,  1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29],\n",
       "         [19, 28, 29,  1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18],\n",
       "         [20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34],\n",
       "         [16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39],\n",
       "         [30, 15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28, 19],\n",
       "         [27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32],\n",
       "         [29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26, 35, 15, 30],\n",
       "         [29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18],\n",
       "         [26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28, 19,  1, 16, 39,  1],\n",
       "         [29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19],\n",
       "         [10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1, 29],\n",
       "         [28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35],\n",
       "         [19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15],\n",
       "         [34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26],\n",
       "         [32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6],\n",
       "         [28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6],\n",
       "         [19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1],\n",
       "         [ 1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26, 35],\n",
       "         [35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28],\n",
       "         [ 1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27],\n",
       "         [29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39],\n",
       "         [29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19],\n",
       "         [34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1],\n",
       "         [39, 27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35],\n",
       "         [ 6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10],\n",
       "         [28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39, 16],\n",
       "         [ 6,  1, 29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29],\n",
       "         [35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28],\n",
       "         [23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1],\n",
       "         [34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32],\n",
       "         [ 6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28],\n",
       "         [ 1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26],\n",
       "         [28, 29,  1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32],\n",
       "         [23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1],\n",
       "         [ 1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27],\n",
       "         [15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5c618f0-78d2-45fa-a519-3d1b8a90e1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB\n",
      "# launch as the following (e.g. in a screen session) and wait ~5 days:\n",
      "# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py\n",
      "\n",
      "wandb_log = True\n",
      "wandb_project = 'gpt_train_string_reversal'\n",
      "# wandb_run_name=''\n",
      "\n",
      "# these make the total batch size be ~0.5M\n",
      "# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520\n",
      "batch_size = 16\n",
      "block_size = 64\n",
      "gradient_accumulation_steps = 5 * 8\n",
      "\n",
      "\n",
      "# this makes total number of tokens be 300B\n",
      "max_iters = 20000\n",
      "lr_decay_iters = 15000\n",
      "\n",
      "# eval stuff\n",
      "eval_interval = 1000\n",
      "eval_iters = 200\n",
      "log_interval = 500\n",
      "\n",
      "# weight decay\n",
      "weight_decay = 1e-1\n",
      "\n",
      "# Model stuff\n",
      "n_embd = 512\n",
      "n_head = 4\n",
      "n_layer = 4\n",
      "\n",
      "learning_rate = 1e-6 # max learning rate\n",
      "min_lr = 1e-7 # learning_rate / 10 usually\n",
      "\n",
      "\n",
      "beta1 = 0.9\n",
      "beta2 = 0.95\n",
      "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
      "decay_lr = True # whether to decay the learning rate\n",
      "warmup_iters = 1000 # how many steps to warm up for\n",
      "lr_decay_iters = 20000 # should be ~= max_iters per Chinchilla\n",
      "\n",
      "\n",
      "bias=False  \n",
      "dropout=0.4\n",
      "\n",
      "out_dir = \"./checkpoints/\"\n"
     ]
    }
   ],
   "source": [
    "config_file = \"train_config.py\"\n",
    "with open(config_file) as f:\n",
    "    print(f.read())\n",
    "exec(open(config_file).read())\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str)) and k.find(\"_\") > 1]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd91006c-0d97-42dc-83c5-9667bb1cc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 41,\n",
       " 'block_size': 64,\n",
       " 'batch_size': 16,\n",
       " 'config_file': 'train_config.py',\n",
       " 'wandb_log': True,\n",
       " 'wandb_project': 'gpt_train_string_reversal',\n",
       " 'gradient_accumulation_steps': 40,\n",
       " 'max_iters': 20000,\n",
       " 'lr_decay_iters': 20000,\n",
       " 'eval_interval': 1000,\n",
       " 'eval_iters': 200,\n",
       " 'log_interval': 500,\n",
       " 'weight_decay': 0.1,\n",
       " 'learning_rate': 1e-06,\n",
       " 'min_lr': 1e-07,\n",
       " 'grad_clip': 1.0,\n",
       " 'decay_lr': True,\n",
       " 'warmup_iters': 1000,\n",
       " 'out_dir': './checkpoints/',\n",
       " 'iter_num': 1000,\n",
       " 'init_from': 'scratch',\n",
       " 'local_iter_num': 500,\n",
       " 'running_mfu': -1.0,\n",
       " 'sample_cursor': 20001,\n",
       " 'micro_step': 39}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79a5f59e-28c1-4ef7-9393-3dcc11fb2d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "model_args['vocab_size'] = vocab_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "faa02a87-660e-41d1-be12-b282efc8835e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt.model import GPTConfig\n",
    "from gpt.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acb15c0c-0bcf-447f-93b1-3c416c591abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 12.61M\n"
     ]
    }
   ],
   "source": [
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c05a7b4a-7620-41b2-bfc5-81d64619b90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(41, 512)\n",
       "    (wpe): Embedding(64, 512)\n",
       "    (drop): Dropout(p=0.4, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.4, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=41, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "419d1f11-d1ee-496d-8cfe-b40af4515d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 18, with 12,636,672 parameters\n",
      "num non-decayed parameter tensors: 9, with 4,608 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "init_from = 'scratch'\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bbf3571-f764-4ddb-9935-93000410d443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zb6qc1kp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>█▁</td></tr><tr><td>iter</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>mfu</td><td>▁▁▁</td></tr><tr><td>time</td><td>█▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>3.38838</td></tr><tr><td>iter</td><td>0</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>mfu</td><td>-100.0</td></tr><tr><td>time</td><td>278.14746</td></tr><tr><td>train/loss</td><td>3.89566</td></tr><tr><td>val/loss</td><td>3.89814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-plant-23</strong> at: <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/zb6qc1kp' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/zb6qc1kp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231207_171346-zb6qc1kp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zb6qc1kp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f41be8c92f54baea8e44e571d437246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112359333330662, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/work/ml_learnings_playground/notebooks/wandb/run-20231207_172333-dx541uow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/dx541uow' target=\"_blank\">wandering-butterfly-24</a></strong> to <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/dx541uow' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/dx541uow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch_classic(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = split\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device_type), y.to(device_type)\n",
    "    return x , y\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    data_dict = {\"train\" : train_data , \"val\" : val_data}\n",
    "    for key in data_dict:\n",
    "        data = data_dict[key]\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        start = random.choice(np.arange(len(data)-eval_iters-1))\n",
    "        i = 0\n",
    "        for k in range(start, start + eval_iters):\n",
    "            # X, Y = get_batch(data[k])\n",
    "            X, Y = get_batch_classic(data)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "            i+=1\n",
    "        out[key] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6271248f-308a-4cfa-b533-079eeaf4386a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 500: loss 3.1984, time 278.98ms, mfu -100.00%\n",
      "step 1000: train loss 3.1706, val loss 3.1660\n",
      "saving checkpoint to ./checkpoints/\n",
      "iter 1000: loss 2.6102, time 11330.25ms, mfu 0.09%\n",
      "iter 1500: loss 2.1281, time 281.28ms, mfu 0.44%\n",
      "step 2000: train loss 4.0651, val loss 4.0731\n",
      "iter 2000: loss 1.7938, time 1402.30ms, mfu 0.47%\n",
      "iter 2500: loss 1.4568, time 282.50ms, mfu 0.78%\n",
      "step 3000: train loss 4.6135, val loss 4.6220\n",
      "iter 3000: loss 1.1881, time 1403.46ms, mfu 0.78%\n",
      "iter 3500: loss 0.8719, time 282.30ms, mfu 1.06%\n",
      "step 4000: train loss 5.2243, val loss 5.2386\n",
      "iter 4000: loss 0.6557, time 1404.45ms, mfu 1.02%\n",
      "iter 4500: loss 0.4472, time 282.40ms, mfu 1.28%\n",
      "step 5000: train loss 5.8124, val loss 5.8276\n",
      "iter 5000: loss 0.3386, time 1402.32ms, mfu 1.22%\n",
      "iter 5500: loss 0.2327, time 283.44ms, mfu 1.46%\n",
      "step 6000: train loss 6.3483, val loss 6.3677\n",
      "iter 6000: loss 0.1596, time 1401.30ms, mfu 1.39%\n",
      "iter 6500: loss 0.1236, time 283.24ms, mfu 1.61%\n",
      "step 7000: train loss 6.8042, val loss 6.8068\n",
      "iter 7000: loss 0.0927, time 1403.66ms, mfu 1.52%\n",
      "iter 7500: loss 0.0703, time 282.87ms, mfu 1.72%\n",
      "step 8000: train loss 7.1568, val loss 7.1572\n",
      "iter 8000: loss 0.0693, time 1401.74ms, mfu 1.62%\n",
      "iter 8500: loss 0.0516, time 283.67ms, mfu 1.82%\n",
      "step 9000: train loss 7.4516, val loss 7.4385\n",
      "iter 9000: loss 0.0406, time 1403.31ms, mfu 1.71%\n",
      "iter 9500: loss 0.0355, time 283.63ms, mfu 1.90%\n",
      "step 10000: train loss 7.6798, val loss 7.6557\n",
      "iter 10000: loss 0.0295, time 1404.99ms, mfu 1.78%\n",
      "iter 10500: loss 0.0272, time 283.19ms, mfu 1.96%\n",
      "step 11000: train loss 7.8287, val loss 7.8420\n",
      "iter 11000: loss 0.0295, time 1404.19ms, mfu 1.83%\n",
      "iter 11500: loss 0.0188, time 283.29ms, mfu 2.01%\n",
      "step 12000: train loss 7.9710, val loss 7.9854\n",
      "iter 12000: loss 0.0235, time 1401.75ms, mfu 1.88%\n",
      "iter 12500: loss 0.0193, time 283.20ms, mfu 2.05%\n",
      "step 13000: train loss 8.0785, val loss 8.0846\n",
      "iter 13000: loss 0.0185, time 1404.58ms, mfu 1.92%\n",
      "iter 13500: loss 0.0221, time 283.14ms, mfu 2.08%\n",
      "step 14000: train loss 8.1594, val loss 8.1687\n",
      "iter 14000: loss 0.0209, time 1401.66ms, mfu 1.95%\n",
      "iter 14500: loss 0.0186, time 283.48ms, mfu 2.11%\n",
      "step 15000: train loss 8.2277, val loss 8.2381\n",
      "iter 15000: loss 0.0209, time 1402.99ms, mfu 1.97%\n",
      "iter 15500: loss 0.0208, time 283.50ms, mfu 2.13%\n",
      "step 16000: train loss 8.3038, val loss 8.2815\n",
      "iter 16000: loss 0.0201, time 1402.00ms, mfu 1.99%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     get_batch_classic(train_data)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     sample_cursor\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# clip the gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/persisted_conda_envs/answerbot/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/persisted_conda_envs/answerbot/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# X, Y = get_batch(train_data[0]) # fetch the very first batch\n",
    "X, Y = get_batch_classic(train_data) # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "sample_cursor = 1\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        logits, loss = model(X, Y)\n",
    "        loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        # X, Y = get_batch(train_data[sample_cursor])\n",
    "        get_batch_classic(train_data)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "        sample_cursor+=1\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        wandb.log({\"eval_loss\": lossf, \"time\": dt*1000 , \"mfu\": running_mfu*100})\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386b6d9-8ca8-4953-b798-e16e4fe2c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1f378-1de2-4ab5-bc52-875d3d80f3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "\n",
    "context = torch.tensor(encode(\"what is happening here _\"), dtype = torch.long , device = device_type).view(1,-1)\n",
    "print(context)\n",
    "print(decode(raw_model.generate(context, max_new_tokens=15)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9a377-793c-49b8-91d1-a460ac7c9f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_answerbot",
   "language": "python",
   "name": "conda_answerbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
