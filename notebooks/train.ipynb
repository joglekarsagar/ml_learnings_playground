{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d30412-a7a3-4a00-9232-e7a8ad6eb750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a428ec1c-a0b1-4c27-b8b3-6e3788b9aae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "with open(\"output_data.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0832fffc-9870-4a11-a760-38607b199d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to have him suddenly_ylneddus mih evah ot^\\n',\n",
       " ' drovethe bristled _ deltsirb ehtevord ^\\n',\n",
       " 'e to thy bridal cham_mahc ladirb yht ot e^\\n',\n",
       " 'd infidels,and in t_t ni dna,sledifni d^\\n',\n",
       " 'her most vile princi_icnirp eliv tsom reh^\\n',\n",
       " 'es upon her peaceful_lufecaep reh nopu se^\\n',\n",
       " ' marriage: when and _ dna nehw :egairram ^\\n',\n",
       " 'brother gloucester h_h retsecuolg rehtorb^\\n',\n",
       " 'alks, my manors that_taht sronam ym ,skla^\\n',\n",
       " 'e bestow a simple in_ni elpmis a wotseb e^\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "181c0558-ce2a-4490-a96a-d66e809a3698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device_type)\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edcbbb37-4905-4d02-9122-81a4e36777d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_vocab = sorted(list(set(\"\".join(sentences))))\n",
    "vocab_size = len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ca940a-532c-4235-82ea-13178429b0e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 23, 23, 1, 34, 22, 19, 32, 19, 13]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(total_vocab) }\n",
    "itos = { i:ch for i,ch in enumerate(total_vocab) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there^\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff227190-e4e0-46ff-9ee7-0bd75d019234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:11<00:00, 87391.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([34, 29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28,\n",
      "        26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36,\n",
      "        15, 22,  1, 29, 34, 13,  0]), tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19, 18,\n",
      "         1, 14,  1, 18, 19, 26, 34, 33, 23, 32, 16,  1, 19, 22, 34, 19, 36, 29,\n",
      "        32, 18,  1, 13,  0]), tensor([19,  1, 34, 29,  1, 34, 22, 39,  1, 16, 32, 23, 18, 15, 26,  1, 17, 22,\n",
      "        15, 27, 14, 27, 15, 22, 17,  1, 26, 15, 18, 23, 32, 16,  1, 39, 22, 34,\n",
      "         1, 29, 34,  1, 19, 13,  0]), tensor([18,  1, 23, 28, 20, 23, 18, 19, 26, 33,  6, 15, 28, 18,  1, 23, 28,  1,\n",
      "        34, 14, 34,  1, 28, 23,  1, 18, 28, 15,  6, 33, 26, 19, 18, 23, 20, 28,\n",
      "        23,  1, 18, 13,  0]), tensor([22, 19, 32,  1, 27, 29, 33, 34,  1, 36, 23, 26, 19,  1, 30, 32, 23, 28,\n",
      "        17, 23, 14, 23, 17, 28, 23, 32, 30,  1, 19, 26, 23, 36,  1, 34, 33, 29,\n",
      "        27,  1, 32, 19, 22, 13,  0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = []\n",
    "for line in tqdm(sentences): \n",
    "    data.append(torch.tensor(encode(line), dtype=torch.long))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0bd9304-4a5a-4877-b404-6997eaf015ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45063e57-50f1-4f2e-9363-d916b2cdb145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19, 18,\n",
      "         1, 14]) tensor([18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19, 18,  1,\n",
      "        14,  1])\n",
      "when input is tensor([1]) the target: 18\n",
      "when input is tensor([ 1, 18]) the target: 32\n",
      "when input is tensor([ 1, 18, 32]) the target: 29\n",
      "when input is tensor([ 1, 18, 32, 29]) the target: 36\n",
      "when input is tensor([ 1, 18, 32, 29, 36]) the target: 19\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19]) the target: 34\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34]) the target: 22\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22]) the target: 19\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19]) the target: 1\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1]) the target: 16\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16]) the target: 32\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32]) the target: 23\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23]) the target: 33\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33]) the target: 34\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34]) the target: 26\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26]) the target: 19\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19]) the target: 18\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19, 18]) the target: 1\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19, 18,\n",
      "         1]) the target: 14\n",
      "when input is tensor([ 1, 18, 32, 29, 36, 19, 34, 22, 19,  1, 16, 32, 23, 33, 34, 26, 19, 18,\n",
      "         1, 14]) the target: 1\n"
     ]
    }
   ],
   "source": [
    "block_size = 20\n",
    "\n",
    "x = train_data[1][:block_size]\n",
    "y = train_data[1][1:block_size+1]\n",
    "print(x, y)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc7f2ab5-eb08-4ddb-b573-a1fff675651f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 9, 3, 4, 9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9, 0, 9, 6, 9,\n",
       "        5, 4, 8, 8, 6, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a90603d-09ed-43e1-a1c5-566b2f141996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 16 # what is the maximum context length for predictions?\n",
    "\n",
    "# poor man's data loader\n",
    "def get_batch(sample):\n",
    "    data = sample\n",
    "    ix = torch.randint(len(data) - block_size + 1, (batch_size,))\n",
    "    x_temps = [data[i:i+block_size] for i in ix]\n",
    "    x = []\n",
    "    for _x in x_temps:\n",
    "        if len(_x) < block_size:\n",
    "            while len(_x) < block_size:\n",
    "                new_element = torch.tensor(encode(\"^\"))\n",
    "                _x = torch.cat((_x,new_element),0)\n",
    "        x.append(_x)\n",
    "    \n",
    "    y_temps = [data[i+1:i+1+block_size] for i in ix]\n",
    "    y = []\n",
    "    for _y in y_temps:\n",
    "        \n",
    "        if len(_y) < block_size:\n",
    "            while len(_y) < block_size:\n",
    "                new_element = torch.tensor(encode(\"^\"))\n",
    "                _y = torch.cat((_y,new_element),0)\n",
    "        y.append(_y)\n",
    "    \n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device_type, non_blocking=True), y.pin_memory().to(device_type, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device_type), y.to(device_type)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd62b4f-dfd6-451c-a7d2-d3ddeb8b1a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34, 29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28,\n",
       "        26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36,\n",
       "        15, 22,  1, 29, 34, 13,  0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a87f66f5-fde6-4527-a953-9fffe1973c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22],\n",
       "         [28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1],\n",
       "         [15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39],\n",
       "         [18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13],\n",
       "         [26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1],\n",
       "         [22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26],\n",
       "         [33, 35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35],\n",
       "         [33, 35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35],\n",
       "         [18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1],\n",
       "         [39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15],\n",
       "         [18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1],\n",
       "         [18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13],\n",
       "         [19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39, 14, 39],\n",
       "         [26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1],\n",
       "         [18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27],\n",
       "         [19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23],\n",
       "         [35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13,  0],\n",
       "         [35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13,  0],\n",
       "         [28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1],\n",
       "         [27,  1, 33, 35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18],\n",
       "         [29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19],\n",
       "         [34, 29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18],\n",
       "         [39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15],\n",
       "         [22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26],\n",
       "         [22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26],\n",
       "         [36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39, 14],\n",
       "         [29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19],\n",
       "         [29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19],\n",
       "         [28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1],\n",
       "         [28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1],\n",
       "         [19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23],\n",
       "         [33, 35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35]],\n",
       "        device='cuda:0'),\n",
       " tensor([[28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1],\n",
       "         [19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29],\n",
       "         [36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39, 14],\n",
       "         [35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13,  0],\n",
       "         [39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19],\n",
       "         [15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39],\n",
       "         [35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33],\n",
       "         [35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33],\n",
       "         [18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27],\n",
       "         [26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22],\n",
       "         [18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27],\n",
       "         [35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13,  0],\n",
       "         [ 1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39, 14, 39, 26],\n",
       "         [39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19],\n",
       "         [19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23],\n",
       "         [28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22],\n",
       "         [33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13,  0, 13],\n",
       "         [33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29, 34, 13,  0, 13],\n",
       "         [19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29],\n",
       "         [ 1, 33, 35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18],\n",
       "         [ 1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28],\n",
       "         [29,  1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19],\n",
       "         [26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22],\n",
       "         [15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39],\n",
       "         [15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39],\n",
       "         [19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28, 26, 39, 14, 39],\n",
       "         [ 1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28],\n",
       "         [ 1, 22, 15, 36, 19,  1, 22, 23, 27,  1, 33, 35, 18, 18, 19, 28],\n",
       "         [19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29],\n",
       "         [19, 18, 18, 35, 33,  1, 27, 23, 22,  1, 19, 36, 15, 22,  1, 29],\n",
       "         [28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33,  1, 27, 23, 22],\n",
       "         [35, 18, 18, 19, 28, 26, 39, 14, 39, 26, 28, 19, 18, 18, 35, 33]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5c618f0-78d2-45fa-a519-3d1b8a90e1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB\n",
      "# launch as the following (e.g. in a screen session) and wait ~5 days:\n",
      "# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py\n",
      "\n",
      "wandb_log = True\n",
      "wandb_project = 'gpt_train_string_reversal'\n",
      "# wandb_run_name=''\n",
      "\n",
      "# these make the total batch size be ~0.5M\n",
      "# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520\n",
      "batch_size = 16\n",
      "block_size = 20\n",
      "gradient_accumulation_steps = 5 * 8\n",
      "\n",
      "# this makes total number of tokens be 300B\n",
      "max_iters = 600000\n",
      "lr_decay_iters = 600000\n",
      "\n",
      "# eval stuff\n",
      "eval_interval = 1000\n",
      "eval_iters = 200\n",
      "log_interval = 10\n",
      "\n",
      "# weight decay\n",
      "weight_decay = 1e-1\n",
      "\n",
      "# Model stuff\n",
      "n_layer = 2\n",
      "n_head = 4\n",
      "n_embd = 32\n",
      "\n",
      "learning_rate = 1e-4 # max learning rate\n",
      "\n",
      "max_iters = 600000 # total number of training iterations\n",
      "\n",
      "beta1 = 0.9\n",
      "beta2 = 0.95\n",
      "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
      "decay_lr = True # whether to decay the learning rate\n",
      "warmup_iters = 2000 # how many steps to warm up for\n",
      "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
      "\n",
      "\n",
      "bias=False  \n",
      "dropout=True\n"
     ]
    }
   ],
   "source": [
    "config_file = \"train_config.py\"\n",
    "with open(config_file) as f:\n",
    "    print(f.read())\n",
    "exec(open(config_file).read())\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd91006c-0d97-42dc-83c5-9667bb1cc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtype': 'bfloat16',\n",
       " 'vocab_size': 41,\n",
       " 'line': 'alls up theshepherd_drehpehseht pu slla^\\n',\n",
       " 'n': 900000,\n",
       " 'block_size': 20,\n",
       " 't': 19,\n",
       " 'batch_size': 16,\n",
       " 'config_file': 'train_config.py',\n",
       " 'wandb_log': True,\n",
       " 'wandb_project': 'gpt_train_string_reversal',\n",
       " 'gradient_accumulation_steps': 40,\n",
       " 'max_iters': 600000,\n",
       " 'lr_decay_iters': 600000,\n",
       " 'eval_interval': 1000,\n",
       " 'eval_iters': 200,\n",
       " 'log_interval': 10,\n",
       " 'weight_decay': 0.1,\n",
       " 'n_layer': 2,\n",
       " 'n_head': 4,\n",
       " 'n_embd': 32,\n",
       " 'learning_rate': 0.0001,\n",
       " 'beta1': 0.9,\n",
       " 'beta2': 0.95,\n",
       " 'grad_clip': 1.0,\n",
       " 'decay_lr': True,\n",
       " 'warmup_iters': 2000,\n",
       " 'bias': False,\n",
       " 'dropout': True}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79a5f59e-28c1-4ef7-9393-3dcc11fb2d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "model_args['vocab_size'] = vocab_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faa02a87-660e-41d1-be12-b282efc8835e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt.model import GPTConfig\n",
    "from gpt.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acb15c0c-0bcf-447f-93b1-3c416c591abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.03M\n"
     ]
    }
   ],
   "source": [
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c05a7b4a-7620-41b2-bfc5-81d64619b90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(41, 32)\n",
       "    (wpe): Embedding(20, 32)\n",
       "    (drop): Dropout(p=True, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=32, out_features=96, bias=False)\n",
       "          (c_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (attn_dropout): Dropout(p=True, inplace=False)\n",
       "          (resid_dropout): Dropout(p=True, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (dropout): Dropout(p=True, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=41, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "419d1f11-d1ee-496d-8cfe-b40af4515d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 10, with 26,528 parameters\n",
      "num non-decayed parameter tensors: 5, with 160 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "init_from = 'scratch'\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bbf3571-f764-4ddb-9935-93000410d443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msagarjoglekar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/work/ml_learnings_playground/notebooks/wandb/run-20231206_165229-bv9gz1s3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/bv9gz1s3' target=\"_blank\">robust-wood-11</a></strong> to <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/bv9gz1s3' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/bv9gz1s3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    data_dict = {\"train\" : train_data , \"val\" : val_data}\n",
    "    for key in data_dict:\n",
    "        data = data_dict[key]\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        start = random.choice(np.arange(len(data)-eval_iters-1))\n",
    "        i = 0\n",
    "        for k in range(start, start + eval_iters):\n",
    "            X, Y = get_batch(data[k])\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "            i+=1\n",
    "        out[key] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6271248f-308a-4cfa-b533-079eeaf4386a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7242, val loss 3.7232\n",
      "iter 0: loss nan, time 1907.16ms, mfu -100.00%\n",
      "iter 10: loss nan, time 134.76ms, mfu 0.01%\n",
      "iter 20: loss nan, time 134.36ms, mfu 0.01%\n",
      "iter 30: loss nan, time 125.34ms, mfu 0.01%\n",
      "iter 40: loss nan, time 125.47ms, mfu 0.01%\n",
      "iter 50: loss nan, time 125.55ms, mfu 0.01%\n",
      "iter 60: loss nan, time 125.58ms, mfu 0.01%\n",
      "iter 70: loss nan, time 126.28ms, mfu 0.01%\n",
      "iter 80: loss nan, time 125.72ms, mfu 0.01%\n",
      "iter 90: loss nan, time 125.62ms, mfu 0.01%\n",
      "iter 100: loss nan, time 124.74ms, mfu 0.01%\n",
      "iter 110: loss nan, time 124.55ms, mfu 0.01%\n",
      "iter 120: loss nan, time 124.70ms, mfu 0.01%\n",
      "iter 130: loss nan, time 125.23ms, mfu 0.01%\n",
      "iter 140: loss nan, time 124.45ms, mfu 0.01%\n",
      "iter 150: loss nan, time 124.59ms, mfu 0.01%\n",
      "iter 160: loss nan, time 124.65ms, mfu 0.01%\n",
      "iter 170: loss nan, time 125.46ms, mfu 0.01%\n",
      "iter 180: loss nan, time 124.48ms, mfu 0.01%\n",
      "iter 190: loss nan, time 124.80ms, mfu 0.01%\n",
      "iter 200: loss nan, time 125.29ms, mfu 0.01%\n",
      "iter 210: loss nan, time 124.54ms, mfu 0.01%\n",
      "iter 220: loss nan, time 124.27ms, mfu 0.01%\n",
      "iter 230: loss nan, time 125.77ms, mfu 0.01%\n",
      "iter 240: loss nan, time 124.92ms, mfu 0.01%\n",
      "iter 250: loss nan, time 124.75ms, mfu 0.01%\n",
      "iter 260: loss nan, time 125.16ms, mfu 0.01%\n",
      "iter 270: loss nan, time 126.27ms, mfu 0.01%\n",
      "iter 280: loss nan, time 124.99ms, mfu 0.01%\n",
      "iter 290: loss nan, time 124.89ms, mfu 0.01%\n",
      "iter 300: loss nan, time 124.66ms, mfu 0.01%\n",
      "iter 310: loss nan, time 125.78ms, mfu 0.01%\n",
      "iter 320: loss nan, time 124.42ms, mfu 0.01%\n",
      "iter 330: loss nan, time 124.29ms, mfu 0.01%\n",
      "iter 340: loss nan, time 124.65ms, mfu 0.01%\n",
      "iter 350: loss nan, time 124.93ms, mfu 0.01%\n",
      "iter 360: loss nan, time 125.27ms, mfu 0.01%\n",
      "iter 370: loss nan, time 124.93ms, mfu 0.01%\n",
      "iter 380: loss nan, time 125.70ms, mfu 0.01%\n",
      "iter 390: loss nan, time 124.57ms, mfu 0.01%\n",
      "iter 400: loss nan, time 124.49ms, mfu 0.01%\n",
      "iter 410: loss nan, time 124.90ms, mfu 0.01%\n",
      "iter 420: loss nan, time 124.94ms, mfu 0.01%\n",
      "iter 430: loss nan, time 124.69ms, mfu 0.01%\n",
      "iter 440: loss nan, time 125.27ms, mfu 0.01%\n",
      "iter 450: loss nan, time 125.10ms, mfu 0.01%\n",
      "iter 460: loss nan, time 124.53ms, mfu 0.01%\n",
      "iter 470: loss nan, time 124.60ms, mfu 0.01%\n",
      "iter 480: loss nan, time 125.10ms, mfu 0.01%\n",
      "iter 490: loss nan, time 125.79ms, mfu 0.01%\n",
      "iter 500: loss nan, time 125.16ms, mfu 0.01%\n",
      "iter 510: loss nan, time 129.22ms, mfu 0.01%\n",
      "iter 520: loss nan, time 126.14ms, mfu 0.01%\n",
      "iter 530: loss nan, time 126.14ms, mfu 0.01%\n",
      "iter 540: loss nan, time 125.43ms, mfu 0.01%\n",
      "iter 550: loss nan, time 128.35ms, mfu 0.01%\n",
      "iter 560: loss nan, time 124.86ms, mfu 0.01%\n",
      "iter 570: loss nan, time 125.75ms, mfu 0.01%\n",
      "iter 580: loss nan, time 124.87ms, mfu 0.01%\n",
      "iter 590: loss nan, time 125.30ms, mfu 0.01%\n",
      "iter 600: loss nan, time 124.86ms, mfu 0.01%\n",
      "iter 610: loss nan, time 125.46ms, mfu 0.01%\n",
      "iter 620: loss nan, time 125.17ms, mfu 0.01%\n",
      "iter 630: loss nan, time 124.59ms, mfu 0.01%\n",
      "iter 640: loss nan, time 124.64ms, mfu 0.01%\n",
      "iter 650: loss nan, time 124.83ms, mfu 0.01%\n",
      "iter 660: loss nan, time 125.03ms, mfu 0.01%\n",
      "iter 670: loss nan, time 127.10ms, mfu 0.01%\n",
      "iter 680: loss nan, time 125.79ms, mfu 0.01%\n",
      "iter 690: loss nan, time 127.13ms, mfu 0.01%\n",
      "iter 700: loss nan, time 126.34ms, mfu 0.01%\n",
      "iter 710: loss nan, time 128.95ms, mfu 0.01%\n",
      "iter 720: loss nan, time 126.12ms, mfu 0.01%\n",
      "iter 730: loss nan, time 124.81ms, mfu 0.01%\n",
      "iter 740: loss nan, time 125.31ms, mfu 0.01%\n",
      "iter 750: loss nan, time 124.79ms, mfu 0.01%\n",
      "iter 760: loss nan, time 124.44ms, mfu 0.01%\n",
      "iter 770: loss nan, time 124.79ms, mfu 0.01%\n",
      "iter 780: loss nan, time 125.42ms, mfu 0.01%\n",
      "iter 790: loss nan, time 125.46ms, mfu 0.01%\n",
      "iter 800: loss nan, time 125.11ms, mfu 0.01%\n",
      "iter 810: loss nan, time 124.76ms, mfu 0.01%\n",
      "iter 820: loss nan, time 125.82ms, mfu 0.01%\n",
      "iter 830: loss nan, time 125.46ms, mfu 0.01%\n",
      "iter 840: loss nan, time 124.71ms, mfu 0.01%\n",
      "iter 850: loss nan, time 125.84ms, mfu 0.01%\n",
      "iter 860: loss nan, time 125.07ms, mfu 0.01%\n",
      "iter 870: loss nan, time 125.02ms, mfu 0.01%\n",
      "iter 880: loss nan, time 124.74ms, mfu 0.01%\n",
      "iter 890: loss nan, time 124.78ms, mfu 0.01%\n",
      "iter 900: loss nan, time 124.78ms, mfu 0.01%\n",
      "iter 910: loss nan, time 125.71ms, mfu 0.01%\n",
      "iter 920: loss nan, time 124.61ms, mfu 0.01%\n",
      "iter 930: loss nan, time 124.76ms, mfu 0.01%\n",
      "iter 940: loss nan, time 125.41ms, mfu 0.01%\n",
      "iter 950: loss nan, time 125.10ms, mfu 0.01%\n",
      "iter 960: loss nan, time 126.62ms, mfu 0.01%\n",
      "iter 970: loss nan, time 124.63ms, mfu 0.01%\n",
      "iter 980: loss nan, time 124.99ms, mfu 0.01%\n",
      "iter 990: loss nan, time 126.23ms, mfu 0.01%\n",
      "step 1000: train loss nan, val loss nan\n",
      "iter 1000: loss nan, time 640.79ms, mfu 0.01%\n",
      "iter 1010: loss nan, time 124.90ms, mfu 0.01%\n",
      "iter 1020: loss nan, time 124.67ms, mfu 0.01%\n",
      "iter 1030: loss nan, time 124.80ms, mfu 0.01%\n",
      "iter 1040: loss nan, time 125.27ms, mfu 0.01%\n",
      "iter 1050: loss nan, time 125.15ms, mfu 0.01%\n",
      "iter 1060: loss nan, time 127.81ms, mfu 0.01%\n",
      "iter 1070: loss nan, time 124.93ms, mfu 0.01%\n",
      "iter 1080: loss nan, time 128.22ms, mfu 0.01%\n",
      "iter 1090: loss nan, time 125.45ms, mfu 0.01%\n",
      "iter 1100: loss nan, time 126.61ms, mfu 0.01%\n",
      "iter 1110: loss nan, time 125.04ms, mfu 0.01%\n",
      "iter 1120: loss nan, time 125.73ms, mfu 0.01%\n",
      "iter 1130: loss nan, time 125.12ms, mfu 0.01%\n",
      "iter 1140: loss nan, time 125.54ms, mfu 0.01%\n",
      "iter 1150: loss nan, time 125.31ms, mfu 0.01%\n",
      "iter 1160: loss nan, time 124.87ms, mfu 0.01%\n",
      "iter 1170: loss nan, time 124.82ms, mfu 0.01%\n",
      "iter 1180: loss nan, time 124.73ms, mfu 0.01%\n",
      "iter 1190: loss nan, time 125.12ms, mfu 0.01%\n",
      "iter 1200: loss nan, time 124.88ms, mfu 0.01%\n",
      "iter 1210: loss nan, time 124.79ms, mfu 0.01%\n",
      "iter 1220: loss nan, time 124.24ms, mfu 0.01%\n",
      "iter 1230: loss nan, time 125.35ms, mfu 0.01%\n",
      "iter 1240: loss nan, time 124.92ms, mfu 0.01%\n",
      "iter 1250: loss nan, time 131.17ms, mfu 0.01%\n",
      "iter 1260: loss nan, time 125.17ms, mfu 0.01%\n",
      "iter 1270: loss nan, time 126.72ms, mfu 0.01%\n",
      "iter 1280: loss nan, time 125.00ms, mfu 0.01%\n",
      "iter 1290: loss nan, time 124.76ms, mfu 0.01%\n",
      "iter 1300: loss nan, time 124.83ms, mfu 0.01%\n",
      "iter 1310: loss nan, time 124.87ms, mfu 0.01%\n",
      "iter 1320: loss nan, time 125.08ms, mfu 0.01%\n",
      "iter 1330: loss nan, time 124.59ms, mfu 0.01%\n",
      "iter 1340: loss nan, time 125.67ms, mfu 0.01%\n",
      "iter 1350: loss nan, time 126.07ms, mfu 0.01%\n",
      "iter 1360: loss nan, time 124.85ms, mfu 0.01%\n",
      "iter 1370: loss nan, time 125.16ms, mfu 0.01%\n",
      "iter 1380: loss nan, time 124.53ms, mfu 0.01%\n",
      "iter 1390: loss nan, time 124.84ms, mfu 0.01%\n",
      "iter 1400: loss nan, time 125.39ms, mfu 0.01%\n",
      "iter 1410: loss nan, time 124.32ms, mfu 0.01%\n",
      "iter 1420: loss nan, time 125.36ms, mfu 0.01%\n",
      "iter 1430: loss nan, time 126.15ms, mfu 0.01%\n",
      "iter 1440: loss nan, time 125.05ms, mfu 0.01%\n",
      "iter 1450: loss nan, time 124.86ms, mfu 0.01%\n",
      "iter 1460: loss nan, time 124.41ms, mfu 0.01%\n",
      "iter 1470: loss nan, time 125.73ms, mfu 0.01%\n",
      "iter 1480: loss nan, time 124.77ms, mfu 0.01%\n",
      "iter 1490: loss nan, time 124.44ms, mfu 0.01%\n",
      "iter 1500: loss nan, time 124.22ms, mfu 0.01%\n",
      "iter 1510: loss nan, time 126.72ms, mfu 0.01%\n",
      "iter 1520: loss nan, time 125.00ms, mfu 0.01%\n",
      "iter 1530: loss nan, time 126.25ms, mfu 0.01%\n",
      "iter 1540: loss nan, time 125.80ms, mfu 0.01%\n",
      "iter 1550: loss nan, time 126.94ms, mfu 0.01%\n",
      "iter 1560: loss nan, time 126.04ms, mfu 0.01%\n",
      "iter 1570: loss nan, time 125.45ms, mfu 0.01%\n",
      "iter 1580: loss nan, time 127.00ms, mfu 0.01%\n",
      "iter 1590: loss nan, time 125.76ms, mfu 0.01%\n",
      "iter 1600: loss nan, time 124.65ms, mfu 0.01%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps \u001b[38;5;66;03m# scale the loss to account for gradient accumulation\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_cursor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[1;32m     49\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     15\u001b[0m             _x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((_x,new_element),\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(_x)\n\u001b[0;32m---> 18\u001b[0m y_temps \u001b[38;5;241m=\u001b[39m [data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix]\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _y \u001b[38;5;129;01min\u001b[39;00m y_temps:\n",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m             _x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((_x,new_element),\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(_x)\n\u001b[0;32m---> 18\u001b[0m y_temps \u001b[38;5;241m=\u001b[39m [\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix]\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _y \u001b[38;5;129;01min\u001b[39;00m y_temps:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "X, Y = get_batch(train_data[0]) # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "sample_cursor = 1\n",
    "while sample_cursor < max_iters:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        logits, loss = model(X, Y)\n",
    "        loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(train_data[sample_cursor])\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "        sample_cursor+=1\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386b6d9-8ca8-4953-b798-e16e4fe2c9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1f378-1de2-4ab5-bc52-875d3d80f3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_answerbot",
   "language": "python",
   "name": "conda_answerbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
