{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d30412-a7a3-4a00-9232-e7a8ad6eb750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a428ec1c-a0b1-4c27-b8b3-6e3788b9aae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "with open(\"output_data_50.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0832fffc-9870-4a11-a760-38607b199d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paulina:true, too true, my lord:if, one by one_eno yb eno ,fi:drol ym ,eurt oot ,eurt:aniluap^\\n',\n",
       " 'r he make his wayunder the colour of his usual ga_ag lausu sih fo ruoloc eht rednuyaw sih ekam eh r^\\n',\n",
       " 'sages, and in the end,having my freedom, boast of_fo tsaob ,modeerf ym gnivah,dne eht ni dna ,segas^\\n',\n",
       " 'ng henry virichard:now, clifford, i have single_elgnis evah i ,droffilc ,won:drahciriv yrneh gn^\\n',\n",
       " \"-damn him. be she honour-flaw'd,i have three daug_guad eerht evah i,d'walf-ruonoh ehs eb .mih nmad-^\\n\",\n",
       " \" clifford; and that's richard duke of york.king _ gnik.kroy fo ekud drahcir s'taht dna ;droffilc ^\\n\",\n",
       " 'and with your queen. i am his cupbearer:if from _ morf fi:reraebpuc sih ma i .neeuq ruoy htiw dna^\\n',\n",
       " 'le:northumberland comes back from bolingbroke.k_k.ekorbgnilob morf kcab semoc dnalrebmuhtron:el^\\n',\n",
       " ' so unlike yourself?petruchio:tedious it were t_t erew ti suoidet:oihcurtep?flesruoy ekilnu os ^\\n',\n",
       " 'y, proud queen, to make thee blush.to tell thee w_w eeht llet ot.hsulb eeht ekam ot ,neeuq duorp ,y^\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c5621a-3f0b-43bb-98c3-cdccb8ab6431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "with open(\"output_data_50.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "text = \"\".join(sentences[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181c0558-ce2a-4490-a96a-d66e809a3698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device_type)\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcbbb37-4905-4d02-9122-81a4e36777d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_vocab = sorted(list(set(\"\".join(sentences))))\n",
    "vocab_size = len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ca940a-532c-4235-82ea-13178429b0e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 23, 23, 1, 34, 22, 19, 32, 19, 13]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(total_vocab) }\n",
    "itos = { i:ch for i,ch in enumerate(total_vocab) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there^\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff227190-e4e0-46ff-9ee7-0bd75d019234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16960.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([30, 15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1,\n",
      "        34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1,\n",
      "        29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39, 16,  1,\n",
      "        19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19,\n",
      "        35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26,\n",
      "        35, 15, 30, 13,  0]), tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28, 18, 19, 32,  1, 34, 22, 19,  1, 17, 29, 26, 29, 35, 32,  1, 29, 20,\n",
      "         1, 22, 23, 33,  1, 35, 33, 35, 15, 26,  1, 21, 15, 14, 15, 21,  1, 26,\n",
      "        15, 35, 33, 35,  1, 33, 23, 22,  1, 20, 29,  1, 32, 35, 29, 26, 29, 17,\n",
      "         1, 19, 22, 34,  1, 32, 19, 18, 28, 35, 39, 15, 37,  1, 33, 23, 22,  1,\n",
      "        19, 25, 15, 27,  1, 19, 22,  1, 32, 13,  0]), tensor([33, 15, 21, 19, 33,  6,  1, 15, 28, 18,  1, 23, 28,  1, 34, 22, 19,  1,\n",
      "        19, 28, 18,  6, 22, 15, 36, 23, 28, 21,  1, 27, 39,  1, 20, 32, 19, 19,\n",
      "        18, 29, 27,  6,  1, 16, 29, 15, 33, 34,  1, 29, 20, 14, 20, 29,  1, 34,\n",
      "        33, 15, 29, 16,  1,  6, 27, 29, 18, 19, 19, 32, 20,  1, 39, 27,  1, 21,\n",
      "        28, 23, 36, 15, 22,  6, 18, 28, 19,  1, 19, 22, 34,  1, 28, 23,  1, 18,\n",
      "        28, 15,  1,  6, 33, 19, 21, 15, 33, 13,  0]), tensor([28, 21,  1, 22, 19, 28, 32, 39,  1, 36, 23, 32, 23, 17, 22, 15, 32, 18,\n",
      "        10, 28, 29, 37,  6,  1, 17, 26, 23, 20, 20, 29, 32, 18,  6,  1, 23,  1,\n",
      "        22, 15, 36, 19,  1, 33, 23, 28, 21, 26, 19, 14, 19, 26, 21, 28, 23, 33,\n",
      "         1, 19, 36, 15, 22,  1, 23,  1,  6, 18, 32, 29, 20, 20, 23, 26, 17,  1,\n",
      "         6, 37, 29, 28, 10, 18, 32, 15, 22, 17, 23, 32, 23, 36,  1, 39, 32, 28,\n",
      "        19, 22,  1, 21, 28, 13,  0]), tensor([ 7, 18, 15, 27, 28,  1, 22, 23, 27,  8,  1, 16, 19,  1, 33, 22, 19,  1,\n",
      "        22, 29, 28, 29, 35, 32,  7, 20, 26, 15, 37,  5, 18,  6, 23,  1, 22, 15,\n",
      "        36, 19,  1, 34, 22, 32, 19, 19,  1, 18, 15, 35, 21, 14, 21, 35, 15, 18,\n",
      "         1, 19, 19, 32, 22, 34,  1, 19, 36, 15, 22,  1, 23,  6, 18,  5, 37, 15,\n",
      "        26, 20,  7, 32, 35, 29, 28, 29, 22,  1, 19, 22, 33,  1, 19, 16,  1,  8,\n",
      "        27, 23, 22,  1, 28, 27, 15, 18,  7, 13,  0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = []\n",
    "for line in tqdm(sentences[:10]): \n",
    "    data.append(torch.tensor(encode(line), dtype=torch.long))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bd9304-4a5a-4877-b404-6997eaf015ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45063e57-50f1-4f2e-9363-d916b2cdb145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28, 18]) tensor([ 1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35, 28,\n",
      "        18, 19])\n",
      "when input is tensor([32]) the target: 1\n",
      "when input is tensor([32,  1]) the target: 22\n",
      "when input is tensor([32,  1, 22]) the target: 19\n",
      "when input is tensor([32,  1, 22, 19]) the target: 1\n",
      "when input is tensor([32,  1, 22, 19,  1]) the target: 27\n",
      "when input is tensor([32,  1, 22, 19,  1, 27]) the target: 15\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15]) the target: 25\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25]) the target: 19\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19]) the target: 1\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1]) the target: 22\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22]) the target: 23\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23]) the target: 33\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33]) the target: 1\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1]) the target: 37\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37]) the target: 15\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15]) the target: 39\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39]) the target: 35\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35]) the target: 28\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28]) the target: 18\n",
      "when input is tensor([32,  1, 22, 19,  1, 27, 15, 25, 19,  1, 22, 23, 33,  1, 37, 15, 39, 35,\n",
      "        28, 18]) the target: 19\n"
     ]
    }
   ],
   "source": [
    "block_size = 20\n",
    "\n",
    "x = train_data[1][:block_size]\n",
    "y = train_data[1][1:block_size+1]\n",
    "print(x, y)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7f2ab5-eb08-4ddb-b573-a1fff675651f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 7, 4, 6, 0, 9, 5, 1, 8, 6, 1, 8, 7, 3, 6, 0, 2, 3, 7, 9, 5, 5, 3,\n",
       "        8, 3, 1, 5, 2, 6, 6, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a90603d-09ed-43e1-a1c5-566b2f141996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 16 # what is the maximum context length for predictions?\n",
    "\n",
    "# poor man's data loader\n",
    "def get_batch(sample):\n",
    "    data = sample\n",
    "    ix = torch.randint(len(data) - block_size + 1, (batch_size,))\n",
    "    x_temps = [data[i:i+block_size] for i in ix]\n",
    "    x = []\n",
    "    for _x in x_temps:\n",
    "        if len(_x) < block_size:\n",
    "            while len(_x) < block_size:\n",
    "                new_element = torch.tensor(encode(\"^\"))\n",
    "                _x = torch.cat((_x,new_element),0)\n",
    "        x.append(_x)\n",
    "    \n",
    "    y_temps = [data[i+1:i+1+block_size] for i in ix]\n",
    "    y = []\n",
    "    for _y in y_temps:\n",
    "        \n",
    "        if len(_y) < block_size:\n",
    "            while len(_y) < block_size:\n",
    "                new_element = torch.tensor(encode(\"^\"))\n",
    "                _y = torch.cat((_y,new_element),0)\n",
    "        y.append(_y)\n",
    "    \n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device_type, non_blocking=True), y.pin_memory().to(device_type, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device_type), y.to(device_type)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd62b4f-dfd6-451c-a7d2-d3ddeb8b1a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1,\n",
       "        34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1,\n",
       "        29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39, 16,  1,\n",
       "        19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19,\n",
       "        35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26,\n",
       "        35, 15, 30, 13,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87f66f5-fde6-4527-a953-9fffe1973c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28],\n",
       "         [39, 27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35],\n",
       "         [29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26, 35, 15],\n",
       "         [34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32],\n",
       "         [ 1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28, 19,  1, 16, 39],\n",
       "         [28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6],\n",
       "         [23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1],\n",
       "         [23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32],\n",
       "         [ 6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10],\n",
       "         [32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23],\n",
       "         [34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20],\n",
       "         [19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1],\n",
       "         [35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39],\n",
       "         [34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26],\n",
       "         [19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15],\n",
       "         [16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39],\n",
       "         [ 1, 29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1],\n",
       "         [28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6],\n",
       "         [10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6],\n",
       "         [ 1, 39, 27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19],\n",
       "         [ 1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34],\n",
       "         [29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39],\n",
       "         [20,  6,  1, 29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28],\n",
       "         [19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15],\n",
       "         [20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34],\n",
       "         [ 1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29],\n",
       "         [19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29],\n",
       "         [29,  1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29],\n",
       "         [19, 28, 29,  1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18],\n",
       "         [20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34],\n",
       "         [16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39],\n",
       "         [30, 15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28, 19],\n",
       "         [27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32],\n",
       "         [29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26, 35, 15, 30],\n",
       "         [29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18],\n",
       "         [26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28, 19,  1, 16, 39,  1],\n",
       "         [29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19],\n",
       "         [10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1, 29],\n",
       "         [28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35],\n",
       "         [19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15],\n",
       "         [34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26],\n",
       "         [32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6],\n",
       "         [28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6],\n",
       "         [19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1],\n",
       "         [ 1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28, 23, 26, 35],\n",
       "         [35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28],\n",
       "         [ 1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27],\n",
       "         [29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39],\n",
       "         [29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19],\n",
       "         [34, 32, 35, 19,  6,  1, 34, 29, 29,  1, 34, 32, 35, 19,  6,  1],\n",
       "         [39, 27,  1,  6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35],\n",
       "         [ 6, 19, 35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10],\n",
       "         [28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29,  1, 39, 16],\n",
       "         [ 6,  1, 29, 28, 19,  1, 16, 39,  1, 29, 28, 19, 14, 19, 28, 29],\n",
       "         [35, 32, 34,  1, 29, 29, 34,  1,  6, 19, 35, 32, 34, 10, 15, 28],\n",
       "         [23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1],\n",
       "         [34, 29, 29,  1, 34, 32, 35, 19,  6,  1, 27, 39,  1, 26, 29, 32],\n",
       "         [ 6,  1, 27, 39,  1, 26, 29, 32, 18, 10, 23, 20,  6,  1, 29, 28],\n",
       "         [ 1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26],\n",
       "         [28, 29,  1, 39, 16,  1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32],\n",
       "         [23, 10, 18, 32, 29, 26,  1, 39, 27,  1,  6, 19, 35, 32, 34,  1],\n",
       "         [ 1, 19, 28, 29,  1,  6, 20, 23, 10, 18, 32, 29, 26,  1, 39, 27],\n",
       "         [15, 35, 26, 23, 28, 15, 10, 34, 32, 35, 19,  6,  1, 34, 29, 29]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c618f0-78d2-45fa-a519-3d1b8a90e1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB\n",
      "# launch as the following (e.g. in a screen session) and wait ~5 days:\n",
      "# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py\n",
      "\n",
      "wandb_log = True\n",
      "wandb_project = 'gpt_train_string_reversal'\n",
      "# wandb_run_name=''\n",
      "\n",
      "# these make the total batch size be ~0.5M\n",
      "# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520\n",
      "batch_size = 16\n",
      "block_size = 64\n",
      "gradient_accumulation_steps = 5 * 8\n",
      "\n",
      "\n",
      "# this makes total number of tokens be 300B\n",
      "max_iters = 50000\n",
      "lr_decay_iters = 40000\n",
      "\n",
      "# eval stuff\n",
      "eval_interval = 1000\n",
      "eval_iters = 200\n",
      "log_interval = 500\n",
      "\n",
      "# weight decay\n",
      "weight_decay = 1e-1\n",
      "\n",
      "# Model stuff\n",
      "n_embd = 512\n",
      "n_head = 4\n",
      "n_layer = 4\n",
      "\n",
      "learning_rate = 1e-4 # max learning rate\n",
      "min_lr = 1e-6 # learning_rate / 10 usually\n",
      "\n",
      "max_iters = 600000 # total number of training iterations\n",
      "\n",
      "beta1 = 0.9\n",
      "beta2 = 0.95\n",
      "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
      "decay_lr = True # whether to decay the learning rate\n",
      "warmup_iters = 2000 # how many steps to warm up for\n",
      "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
      "\n",
      "\n",
      "bias=False  \n",
      "dropout=0.1\n",
      "\n",
      "out_dir = \"./checkpoints/\"\n"
     ]
    }
   ],
   "source": [
    "config_file = \"train_config.py\"\n",
    "with open(config_file) as f:\n",
    "    print(f.read())\n",
    "exec(open(config_file).read())\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str)) and k.find(\"_\") > 1]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd91006c-0d97-42dc-83c5-9667bb1cc6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 41,\n",
       " 'block_size': 64,\n",
       " 'batch_size': 16,\n",
       " 'config_file': 'train_config.py',\n",
       " 'wandb_log': True,\n",
       " 'wandb_project': 'gpt_train_string_reversal',\n",
       " 'gradient_accumulation_steps': 40,\n",
       " 'max_iters': 600000,\n",
       " 'lr_decay_iters': 600000,\n",
       " 'eval_interval': 1000,\n",
       " 'eval_iters': 200,\n",
       " 'log_interval': 500,\n",
       " 'weight_decay': 0.1,\n",
       " 'learning_rate': 0.0001,\n",
       " 'min_lr': 1e-06,\n",
       " 'grad_clip': 1.0,\n",
       " 'decay_lr': True,\n",
       " 'warmup_iters': 2000,\n",
       " 'out_dir': './checkpoints/'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a5f59e-28c1-4ef7-9393-3dcc11fb2d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "model_args['vocab_size'] = vocab_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa02a87-660e-41d1-be12-b282efc8835e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt.model import GPTConfig\n",
    "from gpt.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb15c0c-0bcf-447f-93b1-3c416c591abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 12.61M\n"
     ]
    }
   ],
   "source": [
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05a7b4a-7620-41b2-bfc5-81d64619b90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(41, 512)\n",
       "    (wpe): Embedding(64, 512)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=41, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419d1f11-d1ee-496d-8cfe-b40af4515d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 18, with 12,636,672 parameters\n",
      "num non-decayed parameter tensors: 9, with 4,608 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "init_from = 'scratch'\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbf3571-f764-4ddb-9935-93000410d443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msagarjoglekar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/work/ml_learnings_playground/notebooks/wandb/run-20231207_170739-jribq6z0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/jribq6z0' target=\"_blank\">confused-star-22</a></strong> to <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/jribq6z0' target=\"_blank\">https://wandb.ai/sagarjoglekar/gpt_train_string_reversal/runs/jribq6z0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch_classic(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = split\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device_type), y.to(device_type)\n",
    "    return x , y\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    data_dict = {\"train\" : train_data , \"val\" : val_data}\n",
    "    for key in data_dict:\n",
    "        data = data_dict[key]\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        start = random.choice(np.arange(len(data)-eval_iters-1))\n",
    "        i = 0\n",
    "        for k in range(start, start + eval_iters):\n",
    "            # X, Y = get_batch(data[k])\n",
    "            X, Y = get_batch_classic(data)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "            i+=1\n",
    "        out[key] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271248f-308a-4cfa-b533-079eeaf4386a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8957, val loss 3.8981\n",
      "iter 0: loss 3.8868, time 2899.99ms, mfu -100.00%\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# X, Y = get_batch(train_data[0]) # fetch the very first batch\n",
    "X, Y = get_batch_classic(train_data) # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "sample_cursor = 1\n",
    "while sample_cursor < max_iters:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        logits, loss = model(X, Y)\n",
    "        loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        # X, Y = get_batch(train_data[sample_cursor])\n",
    "        get_batch_classic(train_data)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "        sample_cursor+=1\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        wandb.log({\"eval_loss\": lossf, \"time\": dt*1000 , \"mfu\": running_mfu*100})\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386b6d9-8ca8-4953-b798-e16e4fe2c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1f378-1de2-4ab5-bc52-875d3d80f3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "\n",
    "context = torch.tensor(encode(\"what is happening here _\"), dtype = torch.long , device = device_type).view(1,-1)\n",
    "print(context)\n",
    "print(decode(raw_model.generate(context, max_new_tokens=15)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9a377-793c-49b8-91d1-a460ac7c9f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_answerbot",
   "language": "python",
   "name": "conda_answerbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
