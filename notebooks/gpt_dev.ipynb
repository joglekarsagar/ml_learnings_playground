{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5hjCcLDr2WC",
    "outputId": "ccc60f0c-fd78-4dbe-8598-0512d1036aad",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-06 16:59:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.4’\n",
      "\n",
      "100%[======================================>] 1,115,394   --.-K/s   in 0.008s  \n",
      "\n",
      "2023-12-06 16:59:55 (136 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O6medjfRsLD9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "with open(\"output_data.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()\n",
    "sentences[:10]\n",
    "\n",
    "text = \"\".join(sentences[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !',-.3:;?^_abcdefghijklmnopqrstuvwxyz\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to have him suddenly_ylneddus mih evah ot^\\n drovethe bristled _ deltsirb ehtevord ^\\ne to thy bridal '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 21, 21, 1, 32, 20, 17, 30, 17]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([415582]) torch.int64\n",
      "tensor([32, 27,  1, 20, 13, 34, 17,  1, 20, 21, 25,  1, 31, 33, 16, 16, 17, 26,\n",
      "        24, 37, 12, 37, 24, 26, 17, 16, 16, 33, 31,  1, 25, 21, 20,  1, 17, 34,\n",
      "        13, 20,  1, 27, 32, 11,  0,  1, 16, 30, 27, 34, 17, 32, 20, 17,  1, 14,\n",
      "        30, 21, 31, 32, 24, 17, 16,  1, 12,  1, 16, 17, 24, 32, 31, 21, 30, 14,\n",
      "         1, 17, 20, 32, 17, 34, 27, 30, 16,  1, 11,  0, 17,  1, 32, 27,  1, 32,\n",
      "        20, 37,  1, 14, 30, 21, 16, 13, 24,  1, 15, 20, 13, 25, 12, 25, 13, 20,\n",
      "        15,  1, 24, 13, 16, 21, 30, 14,  1, 37, 20, 32,  1, 27, 32,  1, 17, 11,\n",
      "         0, 16,  1, 21, 26, 18, 21, 16, 17, 24, 31,  4, 13, 26, 16,  1, 21, 26,\n",
      "         1, 32, 12, 32,  1, 26, 21,  1, 16, 26, 13,  4, 31, 24, 17, 16, 21, 18,\n",
      "        26, 21,  1, 16, 11,  0, 20, 17, 30,  1, 25, 27, 31, 32,  1, 34, 21, 24,\n",
      "        17,  1, 28, 30, 21, 26, 15, 21, 12, 21, 15, 26, 21, 30, 28,  1, 17, 24,\n",
      "        21, 34,  1, 32, 31, 27, 25,  1, 30, 17, 20, 11,  0, 17, 31,  1, 33, 28,\n",
      "        27, 26,  1, 20, 17, 30,  1, 28, 17, 13, 15, 17, 18, 33, 24, 12, 24, 33,\n",
      "        18, 17, 15, 13, 17, 28,  1, 30, 17, 20,  1, 26, 27, 28, 33,  1, 31, 17,\n",
      "        11,  0,  1, 25, 13, 30, 30, 21, 13, 19, 17,  8,  1, 35, 20, 17, 26,  1,\n",
      "        13, 26, 16,  1, 12,  1, 16, 26, 13,  1, 26, 17, 20, 35,  1,  8, 17, 19,\n",
      "        13, 21, 30, 30, 13, 25,  1, 11,  0, 14, 30, 27, 32, 20, 17, 30,  1, 19,\n",
      "        24, 27, 33, 15, 17, 31, 32, 17, 30,  1, 20, 12, 20,  1, 30, 17, 32, 31,\n",
      "        17, 15, 33, 27, 24, 19,  1, 30, 17, 20, 32, 27, 30, 14, 11,  0, 13, 24,\n",
      "        23, 31,  4,  1, 25, 37,  1, 25, 13, 26, 27, 30, 31,  1, 32, 20, 13, 32,\n",
      "        12, 32, 13, 20, 32,  1, 31, 30, 27, 26, 13, 25,  1, 37, 25,  1,  4, 31,\n",
      "        23, 24, 13, 11,  0, 17,  1, 14, 17, 31, 32, 27, 35,  1, 13,  1, 31, 21,\n",
      "        25, 28, 24, 17,  1, 21, 26, 12, 26, 21,  1, 17, 24, 28, 25, 21, 31,  1,\n",
      "        13,  1, 35, 27, 32, 31, 17, 14,  1, 17, 11,  0, 15, 13, 28, 33, 24, 17,\n",
      "        32,  8, 27,  1, 20, 17, 13, 34, 17, 26, 31, 12, 31, 26, 17, 34, 13, 17,\n",
      "        20,  1, 27,  8, 32, 17, 24, 33, 28, 13, 15, 11,  0, 32, 17, 17, 24,  6,\n",
      "        35, 20, 21, 32, 17,  5, 14, 17, 13, 30, 16, 31,  1, 20, 12, 20,  1, 31,\n",
      "        16, 30, 13, 17, 14,  5, 17, 32, 21, 20, 35,  6, 24, 17, 17, 32, 11,  0,\n",
      "        32, 27, 26, 19, 33, 17,  1, 15, 27, 33, 24, 16,  1, 26, 17, 34, 17, 30,\n",
      "         1, 24, 12, 24,  1, 30, 17, 34, 17, 26,  1, 16, 24, 33, 27, 15,  1, 17,\n",
      "        33, 19, 26, 27, 32, 11,  0, 30, 17,  1, 33, 26, 18, 27, 30, 32, 33, 26,\n",
      "        13, 32, 17,  1, 32, 20, 13, 26,  1, 12,  1, 26, 13, 20, 32,  1, 17, 32,\n",
      "        13, 26, 33, 32, 30, 27, 18, 26, 33,  1, 17, 30, 11,  0, 30, 16, 17, 24,\n",
      "         1, 13, 26, 16,  1, 14, 27, 36,  4, 35, 20, 21, 15, 20,  1, 12,  1, 20,\n",
      "        15, 21, 20, 35,  4, 36, 27, 14,  1, 16, 26, 13,  1, 24, 17, 16, 30, 11,\n",
      "         0, 16, 31,  1, 32, 27,  1, 20, 17, 13, 24,  1, 13, 19, 13, 21, 26, 32,\n",
      "        20, 13, 12, 13, 20, 32, 26, 21, 13, 19, 13,  1, 24, 13, 17, 20,  1, 27,\n",
      "        32,  1, 31, 16, 11,  0, 21,  1, 35, 21, 24, 24,  1, 15, 27, 33, 26, 32,\n",
      "        17, 30, 18, 17, 21, 32,  1, 32, 12, 32,  1, 32, 21, 17, 18, 30, 17, 32,\n",
      "        26, 33, 27, 15,  1, 24, 24, 21, 35,  1, 21, 11,  0,  1, 19, 27, 27, 16,\n",
      "         1, 32, 20, 21, 31,  1, 27, 31, 32, 17, 26, 32, 13, 32, 21, 12, 21, 32,\n",
      "        13, 32, 26, 17, 32, 31, 27,  1, 31, 21, 20, 32,  1, 16, 27, 27, 19,  1,\n",
      "        11,  0, 17, 17, 31, 15, 27, 30, 17,  1, 13, 26, 16,  1, 32, 35, 27,  1,\n",
      "        37, 17, 13, 30, 12, 30, 13, 17, 37,  1, 27, 35, 32,  1, 16, 26, 13,  1,\n",
      "        17, 30, 27, 15, 31, 17, 17, 11,  0, 17, 16,  1, 13, 31,  1, 21, 18,  1,\n",
      "        37, 27, 33,  1, 31, 24, 17, 28, 32,  6, 12,  6, 32, 28, 17, 24, 31,  1,\n",
      "        33, 27, 37,  1, 18, 21,  1, 31, 13,  1, 16, 17, 11,  0, 17, 25, 21, 26,\n",
      "        19,  1, 32, 27,  1, 14, 17,  1, 25, 27, 31, 32,  1, 35, 20, 21, 12, 21,\n",
      "        20, 35,  1, 32, 31, 27, 25,  1, 17, 14,  1, 27, 32,  1, 19, 26, 21, 25,\n",
      "        17, 11,  0, 33, 31, 17,  4,  1, 16, 27,  1, 37, 27, 33,  1, 32, 20, 21,\n",
      "        26, 23,  4,  1, 21, 12, 21,  1,  4, 23, 26, 21, 20, 32,  1, 33, 27, 37,\n",
      "         1, 27, 16,  1,  4, 17, 31, 33, 11,  0, 17, 26,  4,  1, 14, 30, 13, 32,\n",
      "         4,  1, 21,  3, 24, 24,  1, 28, 24, 13, 19, 33, 12, 33, 19, 13, 24, 28,\n",
      "         1, 24, 24,  3, 21,  1,  4, 32, 13, 30, 14,  1,  4, 26, 17, 11,  0, 31,\n",
      "         1, 32, 20, 21, 31,  1, 19, 27, 24, 16, 17, 26,  1, 15, 30, 27, 35, 26,\n",
      "         1, 12,  1, 26, 35, 27, 30, 15,  1, 26])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f_WIXqxz0lU5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 27,  1, 20, 13, 34, 17,  1, 20])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([32]) the target: 27\n",
      "when input is tensor([32, 27]) the target: 1\n",
      "when input is tensor([32, 27,  1]) the target: 20\n",
      "when input is tensor([32, 27,  1, 20]) the target: 13\n",
      "when input is tensor([32, 27,  1, 20, 13]) the target: 34\n",
      "when input is tensor([32, 27,  1, 20, 13, 34]) the target: 17\n",
      "when input is tensor([32, 27,  1, 20, 13, 34, 17]) the target: 1\n",
      "when input is tensor([32, 27,  1, 20, 13, 34, 17,  1]) the target: 20\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[35, 21, 12, 21, 35, 27, 30, 19],\n",
      "        [26, 21,  1, 17, 30, 13,  1, 37],\n",
      "        [20, 21, 25,  1, 28, 24, 13, 21],\n",
      "        [24, 27, 31, 31,  2, 12,  2, 31]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[21, 12, 21, 35, 27, 30, 19,  1],\n",
      "        [21,  1, 17, 30, 13,  1, 37, 17],\n",
      "        [21, 25,  1, 28, 24, 13, 21, 26],\n",
      "        [27, 31, 31,  2, 12,  2, 31, 31]])\n",
      "----\n",
      "when input is [35] the target: 21\n",
      "when input is [35, 21] the target: 12\n",
      "when input is [35, 21, 12] the target: 21\n",
      "when input is [35, 21, 12, 21] the target: 35\n",
      "when input is [35, 21, 12, 21, 35] the target: 27\n",
      "when input is [35, 21, 12, 21, 35, 27] the target: 30\n",
      "when input is [35, 21, 12, 21, 35, 27, 30] the target: 19\n",
      "when input is [35, 21, 12, 21, 35, 27, 30, 19] the target: 1\n",
      "when input is [26] the target: 21\n",
      "when input is [26, 21] the target: 1\n",
      "when input is [26, 21, 1] the target: 17\n",
      "when input is [26, 21, 1, 17] the target: 30\n",
      "when input is [26, 21, 1, 17, 30] the target: 13\n",
      "when input is [26, 21, 1, 17, 30, 13] the target: 1\n",
      "when input is [26, 21, 1, 17, 30, 13, 1] the target: 37\n",
      "when input is [26, 21, 1, 17, 30, 13, 1, 37] the target: 17\n",
      "when input is [20] the target: 21\n",
      "when input is [20, 21] the target: 25\n",
      "when input is [20, 21, 25] the target: 1\n",
      "when input is [20, 21, 25, 1] the target: 28\n",
      "when input is [20, 21, 25, 1, 28] the target: 24\n",
      "when input is [20, 21, 25, 1, 28, 24] the target: 13\n",
      "when input is [20, 21, 25, 1, 28, 24, 13] the target: 21\n",
      "when input is [20, 21, 25, 1, 28, 24, 13, 21] the target: 26\n",
      "when input is [24] the target: 27\n",
      "when input is [24, 27] the target: 31\n",
      "when input is [24, 27, 31] the target: 31\n",
      "when input is [24, 27, 31, 31] the target: 2\n",
      "when input is [24, 27, 31, 31, 2] the target: 12\n",
      "when input is [24, 27, 31, 31, 2, 12] the target: 2\n",
      "when input is [24, 27, 31, 31, 2, 12, 2] the target: 31\n",
      "when input is [24, 27, 31, 31, 2, 12, 2, 31] the target: 31\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35, 21, 12, 21, 35, 27, 30, 19],\n",
      "        [26, 21,  1, 17, 30, 13,  1, 37],\n",
      "        [20, 21, 25,  1, 28, 24, 13, 21],\n",
      "        [24, 27, 31, 31,  2, 12,  2, 31]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 39])\n",
      "tensor(3.9924, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "she!,p^^wwkhoi!hrguysh;?sgybgqta_ybunvd-l-vo,3!s jpuhoq,wjge\n",
      "vx-n;!g,aid-:^ t;!b-he!?,zyyte\n",
      "ccke!s!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eTyJ8qAaDdiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6618218421936035\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wterro_y aw, tly, ynon r, me\n",
      " bu oc,sa^\n",
      "ueiores fu gh le:ub lerdteh f edia thilirur ce ysipt_neey inichcuahe.napaveprd eikiz_ tn, d ymot_vecendr htsbl ef yd^\n",
      "arm dnihtshcte_? reveo w_oe_s k rt^\n",
      " ua t e q a^\n",
      "fot lp_f-edet drb^\n",
      "re yreb gououo n^\n",
      "io o foroh estose:_toullarg m eruoyaeshia ahtak sehemea !_om ,s'shsndwhseg ahas maerp!ivaromyo sisehia .dnm whs^\n",
      "naieht tha_ ja l eclf:rd o spehcicovllio^\n",
      "i ,d tt r^\n",
      "inkcy en_dll decanin g_ngarof gesit alyer_ toramowhaurae_i_imewihesr t^\n",
      "o^\n",
      "eol^\n",
      ":wi^\n",
      "s,dem\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "86NuXX0fn7ps",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4SNbLq5z3oBw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dRJH6wM_XFfU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.206375 M parameters\n",
      "step 0: train loss 3.9059, val loss 3.9051\n",
      "step 100: train loss 2.8458, val loss 2.8566\n",
      "step 200: train loss 2.7708, val loss 2.7709\n",
      "step 300: train loss 2.7015, val loss 2.7077\n",
      "step 400: train loss 2.6009, val loss 2.6130\n",
      "step 500: train loss 2.5303, val loss 2.5336\n",
      "step 600: train loss 2.4775, val loss 2.4891\n",
      "step 700: train loss 2.4458, val loss 2.4437\n",
      "step 800: train loss 2.3795, val loss 2.3967\n",
      "step 900: train loss 2.3486, val loss 2.3544\n",
      "step 1000: train loss 2.2917, val loss 2.3121\n",
      "step 1100: train loss 2.2566, val loss 2.2654\n",
      "step 1200: train loss 2.2017, val loss 2.2102\n",
      "step 1300: train loss 2.1811, val loss 2.1778\n",
      "step 1400: train loss 2.1530, val loss 2.1594\n",
      "step 1500: train loss 2.1033, val loss 2.1159\n",
      "step 1600: train loss 2.0515, val loss 2.0685\n",
      "step 1700: train loss 2.0137, val loss 2.0372\n",
      "step 1800: train loss 1.9795, val loss 1.9913\n",
      "step 1900: train loss 1.9602, val loss 1.9610\n",
      "step 2000: train loss 1.9433, val loss 1.9440\n",
      "step 2100: train loss 1.9126, val loss 1.9273\n",
      "step 2200: train loss 1.8952, val loss 1.9042\n",
      "step 2300: train loss 1.8544, val loss 1.8765\n",
      "step 2400: train loss 1.8444, val loss 1.8466\n",
      "step 2500: train loss 1.8323, val loss 1.8397\n",
      "step 2600: train loss 1.8143, val loss 1.8334\n",
      "step 2700: train loss 1.8055, val loss 1.8120\n",
      "step 2800: train loss 1.8162, val loss 1.8134\n",
      "step 2900: train loss 1.7830, val loss 1.8048\n",
      "step 3000: train loss 1.7643, val loss 1.7669\n",
      "step 3100: train loss 1.7903, val loss 1.7983\n",
      "step 3200: train loss 1.7640, val loss 1.7812\n",
      "step 3300: train loss 1.7573, val loss 1.7512\n",
      "step 3400: train loss 1.7622, val loss 1.7671\n",
      "step 3500: train loss 1.7382, val loss 1.7510\n",
      "step 3600: train loss 1.7429, val loss 1.7488\n",
      "step 3700: train loss 1.7264, val loss 1.7459\n",
      "step 3800: train loss 1.7307, val loss 1.7497\n",
      "step 3900: train loss 1.7197, val loss 1.7415\n",
      "step 4000: train loss 1.7178, val loss 1.7267\n",
      "step 4100: train loss 1.7127, val loss 1.7259\n",
      "step 4200: train loss 1.6907, val loss 1.7150\n",
      "step 4300: train loss 1.6942, val loss 1.7185\n",
      "step 4400: train loss 1.6866, val loss 1.7112\n",
      "step 4500: train loss 1.6905, val loss 1.6995\n",
      "step 4600: train loss 1.6892, val loss 1.7055\n",
      "step 4700: train loss 1.6874, val loss 1.7040\n",
      "step 4800: train loss 1.6875, val loss 1.6973\n",
      "step 4900: train loss 1.6745, val loss 1.7047\n",
      "step 5000: train loss 1.6823, val loss 1.6923\n",
      "step 5100: train loss 1.6775, val loss 1.6986\n",
      "step 5200: train loss 1.6740, val loss 1.7002\n",
      "step 5300: train loss 1.6533, val loss 1.6827\n",
      "step 5400: train loss 1.6706, val loss 1.6944\n",
      "step 5500: train loss 1.6673, val loss 1.6972\n",
      "step 5600: train loss 1.6537, val loss 1.6795\n",
      "step 5700: train loss 1.6577, val loss 1.6775\n",
      "step 5800: train loss 1.6507, val loss 1.6659\n",
      "step 5900: train loss 1.6596, val loss 1.6761\n",
      "step 6000: train loss 1.6537, val loss 1.6789\n",
      "step 6100: train loss 1.6385, val loss 1.6678\n",
      "step 6200: train loss 1.6443, val loss 1.6789\n",
      "step 6300: train loss 1.6266, val loss 1.6659\n",
      "step 6400: train loss 1.6364, val loss 1.6499\n",
      "step 6500: train loss 1.6363, val loss 1.6600\n",
      "step 6600: train loss 1.6365, val loss 1.6667\n",
      "step 6700: train loss 1.6288, val loss 1.6641\n",
      "step 6800: train loss 1.6121, val loss 1.6281\n",
      "step 6900: train loss 1.6285, val loss 1.6542\n",
      "step 7000: train loss 1.6392, val loss 1.6596\n",
      "step 7100: train loss 1.6101, val loss 1.6548\n",
      "step 7200: train loss 1.6047, val loss 1.6428\n",
      "step 7300: train loss 1.6071, val loss 1.6518\n",
      "step 7400: train loss 1.6086, val loss 1.6448\n",
      "step 7500: train loss 1.5974, val loss 1.6408\n",
      "step 7600: train loss 1.6072, val loss 1.6421\n",
      "step 7700: train loss 1.6134, val loss 1.6507\n",
      "step 7800: train loss 1.6017, val loss 1.6183\n",
      "step 7900: train loss 1.6117, val loss 1.6424\n",
      "step 8000: train loss 1.5951, val loss 1.6306\n",
      "step 8100: train loss 1.5892, val loss 1.6265\n",
      "step 8200: train loss 1.5968, val loss 1.6179\n",
      "step 8300: train loss 1.5937, val loss 1.6363\n",
      "step 8400: train loss 1.5825, val loss 1.6121\n",
      "step 8500: train loss 1.5840, val loss 1.6263\n",
      "step 8600: train loss 1.5839, val loss 1.6161\n",
      "step 8700: train loss 1.5898, val loss 1.6272\n",
      "step 8800: train loss 1.5957, val loss 1.6287\n",
      "step 8900: train loss 1.5782, val loss 1.6132\n",
      "step 9000: train loss 1.5622, val loss 1.6151\n",
      "step 9100: train loss 1.5878, val loss 1.6227\n",
      "step 9200: train loss 1.5844, val loss 1.6245\n",
      "step 9300: train loss 1.5849, val loss 1.6286\n",
      "step 9400: train loss 1.5688, val loss 1.6247\n",
      "step 9500: train loss 1.5717, val loss 1.6241\n",
      "step 9600: train loss 1.5729, val loss 1.6046\n",
      "step 9700: train loss 1.5566, val loss 1.5931\n",
      "step 9800: train loss 1.5591, val loss 1.6074\n",
      "step 9900: train loss 1.5647, val loss 1.6116\n",
      "step 10000: train loss 1.5650, val loss 1.6123\n",
      "step 10100: train loss 1.5822, val loss 1.6108\n",
      "step 10200: train loss 1.5448, val loss 1.6041\n",
      "step 10300: train loss 1.5545, val loss 1.6088\n",
      "step 10400: train loss 1.5653, val loss 1.5990\n",
      "step 10500: train loss 1.5594, val loss 1.6055\n",
      "step 10600: train loss 1.5674, val loss 1.6051\n",
      "step 10700: train loss 1.5501, val loss 1.6060\n",
      "step 10800: train loss 1.5496, val loss 1.6036\n",
      "step 10900: train loss 1.5493, val loss 1.5837\n",
      "step 11000: train loss 1.5435, val loss 1.5969\n",
      "step 11100: train loss 1.5527, val loss 1.6079\n",
      "step 11200: train loss 1.5451, val loss 1.5940\n",
      "step 11300: train loss 1.5487, val loss 1.5969\n",
      "step 11400: train loss 1.5377, val loss 1.5943\n",
      "step 11500: train loss 1.5353, val loss 1.5801\n",
      "step 11600: train loss 1.5406, val loss 1.5896\n",
      "step 11700: train loss 1.5247, val loss 1.5907\n",
      "step 11800: train loss 1.5435, val loss 1.6059\n",
      "step 11900: train loss 1.5356, val loss 1.5824\n",
      "step 12000: train loss 1.5399, val loss 1.5912\n",
      "step 12100: train loss 1.5398, val loss 1.5839\n",
      "step 12200: train loss 1.5280, val loss 1.5796\n",
      "step 12300: train loss 1.5308, val loss 1.5656\n",
      "step 12400: train loss 1.5405, val loss 1.5879\n",
      "step 12500: train loss 1.5306, val loss 1.5860\n",
      "step 12600: train loss 1.5234, val loss 1.5716\n",
      "step 12700: train loss 1.5349, val loss 1.5745\n",
      "step 12800: train loss 1.5370, val loss 1.5832\n",
      "step 12900: train loss 1.5329, val loss 1.5725\n",
      "step 13000: train loss 1.5310, val loss 1.5706\n",
      "step 13100: train loss 1.5348, val loss 1.5946\n",
      "step 13200: train loss 1.5053, val loss 1.5704\n",
      "step 13300: train loss 1.5164, val loss 1.5666\n",
      "step 13400: train loss 1.5194, val loss 1.5797\n",
      "step 13500: train loss 1.5240, val loss 1.5768\n",
      "step 13600: train loss 1.5227, val loss 1.5724\n",
      "step 13700: train loss 1.5134, val loss 1.5720\n",
      "step 13800: train loss 1.5097, val loss 1.5823\n",
      "step 13900: train loss 1.5200, val loss 1.5756\n",
      "step 14000: train loss 1.5175, val loss 1.5697\n",
      "step 14100: train loss 1.5192, val loss 1.5543\n",
      "step 14200: train loss 1.5183, val loss 1.5818\n",
      "step 14300: train loss 1.5158, val loss 1.5598\n",
      "step 14400: train loss 1.5101, val loss 1.5594\n",
      "step 14500: train loss 1.5169, val loss 1.5671\n",
      "step 14600: train loss 1.5237, val loss 1.5622\n",
      "step 14700: train loss 1.5010, val loss 1.5761\n",
      "step 14800: train loss 1.4864, val loss 1.5489\n",
      "step 14900: train loss 1.5042, val loss 1.5715\n",
      "step 15000: train loss 1.5026, val loss 1.5672\n",
      "step 15100: train loss 1.4876, val loss 1.5718\n",
      "step 15200: train loss 1.4946, val loss 1.5630\n",
      "step 15300: train loss 1.4925, val loss 1.5583\n",
      "step 15400: train loss 1.5055, val loss 1.5517\n",
      "step 15500: train loss 1.4913, val loss 1.5649\n",
      "step 15600: train loss 1.4972, val loss 1.5666\n",
      "step 15700: train loss 1.4878, val loss 1.5607\n",
      "step 15800: train loss 1.5045, val loss 1.5746\n",
      "step 15900: train loss 1.4993, val loss 1.5564\n",
      "step 16000: train loss 1.4893, val loss 1.5577\n",
      "step 16100: train loss 1.4964, val loss 1.5628\n",
      "step 16200: train loss 1.4853, val loss 1.5667\n",
      "step 16300: train loss 1.5021, val loss 1.5659\n",
      "step 16400: train loss 1.4927, val loss 1.5586\n",
      "step 16500: train loss 1.4864, val loss 1.5515\n",
      "step 16600: train loss 1.5063, val loss 1.5807\n",
      "step 16700: train loss 1.4917, val loss 1.5512\n",
      "step 16800: train loss 1.4888, val loss 1.5471\n",
      "step 16900: train loss 1.4927, val loss 1.5685\n",
      "step 17000: train loss 1.4885, val loss 1.5537\n",
      "step 17100: train loss 1.4881, val loss 1.5533\n",
      "step 17200: train loss 1.4927, val loss 1.5565\n",
      "step 17300: train loss 1.4837, val loss 1.5528\n",
      "step 17400: train loss 1.4923, val loss 1.5505\n",
      "step 17500: train loss 1.4876, val loss 1.5473\n",
      "step 17600: train loss 1.4911, val loss 1.5521\n",
      "step 17700: train loss 1.4861, val loss 1.5502\n",
      "step 17800: train loss 1.4854, val loss 1.5636\n",
      "step 17900: train loss 1.4794, val loss 1.5376\n",
      "step 18000: train loss 1.4794, val loss 1.5528\n",
      "step 18100: train loss 1.4865, val loss 1.5471\n",
      "step 18200: train loss 1.4698, val loss 1.5577\n",
      "step 18300: train loss 1.4839, val loss 1.5637\n",
      "step 18400: train loss 1.4704, val loss 1.5468\n",
      "step 18500: train loss 1.4676, val loss 1.5487\n",
      "step 18600: train loss 1.4704, val loss 1.5417\n",
      "step 18700: train loss 1.4844, val loss 1.5475\n",
      "step 18800: train loss 1.4736, val loss 1.5508\n",
      "step 18900: train loss 1.4796, val loss 1.5450\n",
      "step 19000: train loss 1.4668, val loss 1.5578\n",
      "step 19100: train loss 1.4747, val loss 1.5486\n",
      "step 19200: train loss 1.4643, val loss 1.5320\n",
      "step 19300: train loss 1.4707, val loss 1.5479\n",
      "step 19400: train loss 1.4636, val loss 1.5434\n",
      "step 19500: train loss 1.4663, val loss 1.5446\n",
      "step 19600: train loss 1.4792, val loss 1.5575\n",
      "step 19700: train loss 1.4668, val loss 1.5439\n",
      "step 19800: train loss 1.4818, val loss 1.5524\n",
      "step 19900: train loss 1.4723, val loss 1.5528\n",
      "step 20000: train loss 1.4720, val loss 1.5461\n",
      "step 20100: train loss 1.4665, val loss 1.5436\n",
      "step 20200: train loss 1.4546, val loss 1.5361\n",
      "step 20300: train loss 1.4547, val loss 1.5431\n",
      "step 20400: train loss 1.4687, val loss 1.5493\n",
      "step 20500: train loss 1.4698, val loss 1.5338\n",
      "step 20600: train loss 1.4632, val loss 1.5409\n",
      "step 20700: train loss 1.4509, val loss 1.5580\n",
      "step 20800: train loss 1.4586, val loss 1.5456\n",
      "step 20900: train loss 1.4663, val loss 1.5356\n",
      "step 21000: train loss 1.4685, val loss 1.5370\n",
      "step 21100: train loss 1.4708, val loss 1.5304\n",
      "step 21200: train loss 1.4741, val loss 1.5461\n",
      "step 21300: train loss 1.4511, val loss 1.5508\n",
      "step 21400: train loss 1.4529, val loss 1.5326\n",
      "step 21500: train loss 1.4535, val loss 1.5494\n",
      "step 21600: train loss 1.4649, val loss 1.5372\n",
      "step 21700: train loss 1.4476, val loss 1.5279\n",
      "step 21800: train loss 1.4546, val loss 1.5413\n",
      "step 21900: train loss 1.4473, val loss 1.5454\n",
      "step 22000: train loss 1.4511, val loss 1.5326\n",
      "step 22100: train loss 1.4563, val loss 1.5328\n",
      "step 22200: train loss 1.4504, val loss 1.5392\n",
      "step 22300: train loss 1.4573, val loss 1.5416\n",
      "step 22400: train loss 1.4514, val loss 1.5340\n",
      "step 22500: train loss 1.4362, val loss 1.5291\n",
      "step 22600: train loss 1.4438, val loss 1.5328\n",
      "step 22700: train loss 1.4617, val loss 1.5340\n",
      "step 22800: train loss 1.4504, val loss 1.5391\n",
      "step 22900: train loss 1.4507, val loss 1.5375\n",
      "step 23000: train loss 1.4514, val loss 1.5364\n",
      "step 23100: train loss 1.4442, val loss 1.5334\n",
      "step 23200: train loss 1.4557, val loss 1.5387\n",
      "step 23300: train loss 1.4549, val loss 1.5486\n",
      "step 23400: train loss 1.4378, val loss 1.5405\n",
      "step 23500: train loss 1.4422, val loss 1.5477\n",
      "step 23600: train loss 1.4511, val loss 1.5356\n",
      "step 23700: train loss 1.4364, val loss 1.5377\n",
      "step 23800: train loss 1.4409, val loss 1.5301\n",
      "step 23900: train loss 1.4405, val loss 1.5282\n",
      "step 24000: train loss 1.4448, val loss 1.5240\n",
      "step 24100: train loss 1.4551, val loss 1.5289\n",
      "step 24200: train loss 1.4474, val loss 1.5365\n",
      "step 24300: train loss 1.4306, val loss 1.5329\n",
      "step 24400: train loss 1.4430, val loss 1.5326\n",
      "step 24500: train loss 1.4418, val loss 1.5170\n",
      "step 24600: train loss 1.4448, val loss 1.5271\n",
      "step 24700: train loss 1.4333, val loss 1.5271\n",
      "step 24800: train loss 1.4397, val loss 1.5392\n",
      "step 24900: train loss 1.4325, val loss 1.5257\n",
      "step 25000: train loss 1.4529, val loss 1.5320\n",
      "step 25100: train loss 1.4407, val loss 1.5394\n",
      "step 25200: train loss 1.4224, val loss 1.5237\n",
      "step 25300: train loss 1.4435, val loss 1.5322\n",
      "step 25400: train loss 1.4432, val loss 1.5170\n",
      "step 25500: train loss 1.4252, val loss 1.5161\n",
      "step 25600: train loss 1.4304, val loss 1.5401\n",
      "step 25700: train loss 1.4311, val loss 1.5240\n",
      "step 25800: train loss 1.4312, val loss 1.5373\n",
      "step 25900: train loss 1.4384, val loss 1.5172\n",
      "step 26000: train loss 1.4288, val loss 1.5345\n",
      "step 26100: train loss 1.4334, val loss 1.5243\n",
      "step 26200: train loss 1.4266, val loss 1.5242\n",
      "step 26300: train loss 1.4276, val loss 1.5373\n",
      "step 26400: train loss 1.4353, val loss 1.5122\n",
      "step 26500: train loss 1.4338, val loss 1.5248\n",
      "step 26600: train loss 1.4346, val loss 1.5137\n",
      "step 26700: train loss 1.4312, val loss 1.5276\n",
      "step 26800: train loss 1.4329, val loss 1.5292\n",
      "step 26900: train loss 1.4262, val loss 1.5243\n",
      "step 27000: train loss 1.4269, val loss 1.5283\n",
      "step 27100: train loss 1.4453, val loss 1.5277\n",
      "step 27200: train loss 1.4224, val loss 1.5153\n",
      "step 27300: train loss 1.4162, val loss 1.5391\n",
      "step 27400: train loss 1.4370, val loss 1.5332\n",
      "step 27500: train loss 1.4331, val loss 1.5354\n",
      "step 27600: train loss 1.4361, val loss 1.5235\n",
      "step 27700: train loss 1.4427, val loss 1.5433\n",
      "step 27800: train loss 1.4259, val loss 1.5359\n",
      "step 27900: train loss 1.4270, val loss 1.5119\n",
      "step 28000: train loss 1.4208, val loss 1.5378\n",
      "step 28100: train loss 1.4316, val loss 1.5449\n",
      "step 28200: train loss 1.4124, val loss 1.5449\n",
      "step 28300: train loss 1.4304, val loss 1.5286\n",
      "step 28400: train loss 1.4210, val loss 1.5282\n",
      "step 28500: train loss 1.4222, val loss 1.5335\n",
      "step 28600: train loss 1.4236, val loss 1.5136\n",
      "step 28700: train loss 1.4202, val loss 1.5253\n",
      "step 28800: train loss 1.4192, val loss 1.5399\n",
      "step 28900: train loss 1.4243, val loss 1.5155\n",
      "step 29000: train loss 1.4204, val loss 1.5136\n",
      "step 29100: train loss 1.4205, val loss 1.5297\n",
      "step 29200: train loss 1.4168, val loss 1.5317\n",
      "step 29300: train loss 1.4110, val loss 1.5261\n",
      "step 29400: train loss 1.4300, val loss 1.5350\n",
      "step 29500: train loss 1.4223, val loss 1.5204\n",
      "step 29600: train loss 1.4177, val loss 1.5318\n",
      "step 29700: train loss 1.4141, val loss 1.5309\n",
      "step 29800: train loss 1.4122, val loss 1.5344\n",
      "step 29900: train loss 1.4095, val loss 1.5363\n",
      "step 30000: train loss 1.4123, val loss 1.5297\n",
      "step 30100: train loss 1.4113, val loss 1.5358\n",
      "step 30200: train loss 1.4218, val loss 1.5203\n",
      "step 30300: train loss 1.4216, val loss 1.5250\n",
      "step 30400: train loss 1.4050, val loss 1.5273\n",
      "step 30500: train loss 1.4088, val loss 1.5156\n",
      "step 30600: train loss 1.4032, val loss 1.5361\n",
      "step 30700: train loss 1.4186, val loss 1.5256\n",
      "step 30800: train loss 1.4048, val loss 1.5169\n",
      "step 30900: train loss 1.4072, val loss 1.5142\n",
      "step 31000: train loss 1.4191, val loss 1.5256\n",
      "step 31100: train loss 1.4138, val loss 1.5250\n",
      "step 31200: train loss 1.4121, val loss 1.5411\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 50000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "sentences = [] \n",
    "with open(\"output_data.txt\",\"r\") as f:\n",
    "    sentences = f.readlines()\n",
    "sentences[:10]\n",
    "\n",
    "text = \"\".join(sentences[:10000])\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 20, 21, 31, 1, 21, 31, 1, 13, 1, 32, 17, 31, 32, 12]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"this is a test_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "fjjvMifYZf7x",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32, 20, 21, 31,  1, 21, 31,  1, 13,  1, 32, 17, 31, 32, 12]],\n",
      "       device='cuda:0')\n",
      "this is a test_tset a si siht deva^\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context = torch.tensor(encode(\"this is a test_\"), dtype = torch.long , device = device).view(1,-1)\n",
    "print(context)\n",
    "print(decode(m.generate(context, max_new_tokens=20)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint to ./checkpoints/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "out_dir = \"./checkpoints/\"\n",
    "checkpoint = {\n",
    "                    'model': m.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }\n",
    "print(f\"saving checkpoint to {out_dir}\")\n",
    "torch.save(checkpoint, os.path.join(out_dir, 'string_reverse_v1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(m , os.path.join(out_dir, 'string_reverse_v1_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_answerbot",
   "language": "python",
   "name": "conda_answerbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
